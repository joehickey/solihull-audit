---
title: "solihull-audit data analysis"
author: "Joe Hickey"
date: 23/10/2017
output:
  word_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = FALSE, echo = FALSE)
```

```{r}
setwd("/Users/joe/R/solihull-audit")
```

```{r preparation, include=FALSE}

library(ggplot2)
library(gplots)
library(car)
library(tidyr)
library(plyr)
library(nlme)
library(multcomp)
library(dplyr)
library(psy)
library(Hmisc)
library(knitr)

# ToDo:
# Make PROQOL variables right in 'long' format datset
# Sort out variables for training feedback questionnaire

######################
## DATA PREPARATION ##
######################

# Define the variable types
varTypes <-
  c(
    "character",
    "factor",
    "factor",
    "integer",
    "factor",
    "character",
    "character",
    "factor",
    "integer",
    "integer",
    rep("integer", (60 + 14 + 14 + 14 + 14 + 23)),
    rep("character", 7),
    rep("integer", 23),
    rep("character", 7),
    rep("integer", 15),
    rep("character", 7)
  )

# Read the data
dat <-
  read.csv("solihull_eval_db_20160728.csv",
           colClasses = varTypes)

# Recode job into four levels
jobCatVarsAll  <-
  c(
    "coach",
    "FSW",
    "NN",
    "OT",
    "OTA",
    "other",
    "physio",
    "SLT",
    "SLTA",
    "SW",
    "TA",
    "teacher"
  )
targetSet <- c("FSW", "teacher", "TA", "SLT", "other")
dat$jobCat <- ifelse(dat$jobCat %in% targetSet, dat$jobCat, "other")
jobCatVarsFour  <- c("other", "FSW", "SLT", "teacher", "TA")
dat$jobCat <- factor(dat$jobCat, levels = jobCatVarsFour)

# Recode Knowledge Skills and Confidence Questionnaire variables
SKCQ1Conf <-
  (dat$SKCQ1_1 + dat$SKCQ1_2 + dat$SKCQ1_3 + dat$SKCQ1_4) / 4
SKCQ1Und <-
  (dat$SKCQ1_5 + dat$SKCQ1_6 + dat$SKCQ1_7 + dat$SKCQ1_8 + dat$SKCQ1_9) / 5
SKCQ1Use <-
  (dat$SKCQ1_10 + dat$SKCQ1_11 + dat$SKCQ1_12 + dat$SKCQ1_13 + dat$SKCQ1_14) / 5
SKCQ2Conf <-
  (dat$SKCQ2_1 + dat$SKCQ2_2 + dat$SKCQ2_3 + dat$SKCQ2_4) / 4
SKCQ2Und <-
  (dat$SKCQ2_5 + dat$SKCQ2_6 + dat$SKCQ2_7 + dat$SKCQ2_8 + dat$SKCQ2_9) / 5
SKCQ2Use <-
  (dat$SKCQ2_10 + dat$SKCQ2_11 + dat$SKCQ2_12 + dat$SKCQ2_13 + dat$SKCQ2_14) / 5
SKCQ3Conf <-
  (dat$SKCQ3_1 + dat$SKCQ3_2 + dat$SKCQ3_3 + dat$SKCQ3_4) / 4
SKCQ3Und <-
  (dat$SKCQ3_5 + dat$SKCQ3_6 + dat$SKCQ3_7 + dat$SKCQ3_8 + dat$SKCQ3_9) / 5
SKCQ3Use <-
  (dat$SKCQ3_10 + dat$SKCQ3_11 + dat$SKCQ3_12 + dat$SKCQ3_13 + dat$SKCQ3_14) / 5
SKCQ4Conf <-
  (dat$SKCQ4_1 + dat$SKCQ4_2 + dat$SKCQ4_3 + dat$SKCQ4_4) / 4
SKCQ4Und <-
  (dat$SKCQ4_5 + dat$SKCQ4_6 + dat$SKCQ4_7 + dat$SKCQ4_8 + dat$SKCQ4_9) / 5
SKCQ4Use <-
  (dat$SKCQ4_10 + dat$SKCQ4_11 + dat$SKCQ4_12 + dat$SKCQ4_13 + dat$SKCQ4_14) / 5
# Add new SKCQ variables to main dataset
dat$SKCQ1Conf <- SKCQ1Conf
dat$SKCQ2Conf <- SKCQ2Conf
dat$SKCQ3Conf <- SKCQ3Conf
dat$SKCQ4Conf <- SKCQ4Conf
dat$SKCQ1Und <- SKCQ1Und
dat$SKCQ2Und <- SKCQ2Und
dat$SKCQ3Und <- SKCQ3Und
dat$SKCQ4Und <- SKCQ4Und
dat$SKCQ1Use <- SKCQ1Use
dat$SKCQ2Use <- SKCQ2Use
dat$SKCQ3Use <- SKCQ3Use
dat$SKCQ4Use <- SKCQ4Use
# Make summary SKCQ variables
SKCQConf <- data.frame(SKCQ1Conf, SKCQ2Conf, SKCQ3Conf, SKCQ4Conf)
SKCQUnd <- data.frame(SKCQ1Und, SKCQ2Und, SKCQ3Und, SKCQ4Und)
SKCQUse <- data.frame(SKCQ1Use, SKCQ2Use, SKCQ3Use, SKCQ4Use)
SKCQAll <-
  data.frame(
    SKCQ1Conf,
    SKCQ2Conf,
    SKCQ3Conf,
    SKCQ4Conf,
    SKCQ1Und,
    SKCQ2Und,
    SKCQ3Und,
    SKCQ4Und,
    SKCQ1Use,
    SKCQ2Use,
    SKCQ3Use,
    SKCQ4Use
  )

# Recode Professional Quality of Life Scale
PROQOLPre_1r <-
  recode(
    dat$PROQOLPre_1,
    '1' = 5,
    '2' = 4,
    '3' = 3,
    '4' = 2,
    '5' = 1
  )
PROQOLPre_4r <-
  recode(
    dat$PROQOLPre_4,
    '1' = 5,
    '2' = 4,
    '3' = 3,
    '4' = 2,
    '5' = 1
  )
PROQOLPre_15r <-
  recode(
    dat$PROQOLPre_15,
    '1' = 5,
    '2' = 4,
    '3' = 3,
    '4' = 2,
    '5' = 1
  )
PROQOLPre_17r <-
  recode(
    dat$PROQOLPre_17,
    '1' = 5,
    '2' = 4,
    '3' = 3,
    '4' = 2,
    '5' = 1
  )
PROQOLPre_29r <-
  recode(
    dat$PROQOLPre_29,
    '1' = 5,
    '2' = 4,
    '3' = 3,
    '4' = 2,
    '5' = 1
  )
proQolPreCS <-
  dat$PROQOLPre_3 + dat$PROQOLPre_6 + dat$PROQOLPre_12 + dat$PROQOLPre_16 + dat$PROQOLPre_18 + dat$PROQOLPre_20 + dat$PROQOLPre_22 + dat$PROQOLPre_24 + dat$PROQOLPre_27 + dat$PROQOLPre_30
proQolPreBO <-
  PROQOLPre_1r + PROQOLPre_4r + dat$PROQOLPre_8 + dat$PROQOLPre_10 + PROQOLPre_15r + PROQOLPre_17r + dat$PROQOLPre_19 + dat$PROQOLPre_21 + dat$PROQOLPre_26 + PROQOLPre_29r
proQolPreSTS <-
  dat$PROQOLPre_2 + dat$PROQOLPre_5 + dat$PROQOLPre_7 + dat$PROQOLPre_9 + dat$PROQOLPre_11 + dat$PROQOLPre_13 + dat$PROQOLPre_14 + dat$PROQOLPre_23 + dat$PROQOLPre_25 + dat$PROQOLPre_28
# Add new PROQOL pre-training variables to main dataset
dat$proQolPreCS <- proQolPreCS
dat$proQolPreBO <- proQolPreBO
dat$proQolPreSTS <- proQolPreSTS

PROQOLPost_1r <-
  recode(
    dat$PROQOLPost_1,
    '1' = 5,
    '2' = 4,
    '3' = 3,
    '4' = 2,
    '5' = 1
  )
PROQOLPost_4r <-
  recode(
    dat$PROQOLPost_4,
    '1' = 5,
    '2' = 4,
    '3' = 3,
    '4' = 2,
    '5' = 1
  )
PROQOLPost_15r <-
  recode(
    dat$PROQOLPost_15,
    '1' = 5,
    '2' = 4,
    '3' = 3,
    '4' = 2,
    '5' = 1
  )
PROQOLPost_17r <-
  recode(
    dat$PROQOLPost_17,
    '1' = 5,
    '2' = 4,
    '3' = 3,
    '4' = 2,
    '5' = 1
  )
PROQOLPost_29r <-
  recode(
    dat$PROQOLPost_29,
    '1' = 5,
    '2' = 4,
    '3' = 3,
    '4' = 2,
    '5' = 1
  )
proQolPostCS <-
  dat$PROQOLPost_3 + dat$PROQOLPost_6 + dat$PROQOLPost_12 + dat$PROQOLPost_16 + dat$PROQOLPost_18 + dat$PROQOLPost_20 + dat$PROQOLPost_22 + dat$PROQOLPost_24 + dat$PROQOLPost_27 + dat$PROQOLPost_30
proQolPostBO <-
  PROQOLPost_1r + PROQOLPost_4r + dat$PROQOLPost_8 + dat$PROQOLPost_10 + PROQOLPost_15r + PROQOLPost_17r + dat$PROQOLPost_19 + dat$PROQOLPost_21 + dat$PROQOLPost_26 + PROQOLPost_29r
proQolPostSTS <-
  dat$PROQOLPost_2 + dat$PROQOLPost_5 + dat$PROQOLPost_7 + dat$PROQOLPost_9 + dat$PROQOLPost_11 + dat$PROQOLPost_13 + dat$PROQOLPost_14 + dat$PROQOLPost_23 + dat$PROQOLPost_25 + dat$PROQOLPost_28
# Add new PROQOL post-training variables to main dataset
dat$proQolPostCS <- proQolPostCS
dat$proQolPostBO <- proQolPostBO
dat$proQolPostSTS <- proQolPostSTS

# PROQOL make data frame of all pre and post scores for correlation analysis
proQolAll <-
  data.frame(proQolPreCS,
             proQolPreBO,
             proQolPreSTS,
             proQolPostCS,
             proQolPostBO,
             proQolPostSTS)

# Centre and scale ProQOL variables to produce Z-scores
proQolPreCSZ <- c(scale(proQolPreCS))
proQolPreBOZ <- c(scale(proQolPreBO))
proQolPreSTSZ <- c(scale(proQolPreSTS))
proQolPostCSZ <- c(scale(proQolPostCS))
proQolPostBOZ <- c(scale(proQolPostBO))
proQolPostSTSZ <- c(scale(proQolPostSTS))

# Convert ProQOL Z-scores to T-scores
proQolPreCST <- (proQolPreCSZ * 10) + 50
proQolPreBOT <- (proQolPreBOZ * 10) + 50
proQolPreSTST <- (proQolPreSTSZ * 10) + 50
proQolPostCST <- (proQolPostCSZ * 10) + 50
proQolPostBOT <- (proQolPostBOZ * 10) + 50
proQolPostSTST <- (proQolPostSTSZ * 10) + 50

# Calculate change scores
dat$SKCQConfChg <- SKCQ4Conf - SKCQ1Conf
dat$SKCQUndChg <- SKCQ4Und - SKCQ1Und
dat$SKCQUseChg <- SKCQ4Use - SKCQ1Use
dat$proQolCSChg <- proQolPostCS - proQolPreCS
dat$proQolBOChg <- proQolPostBO - proQolPreBO
dat$proQolSTSChg <- proQolPostSTS - proQolPreSTS

# Make a data.frame in 'long' format to assist repeated measures analysis
datLong <- dat
datLong[, grepl(("^SKCQ[0-4][CU]"), names(dat))] %>% # Select only the 12 needed columns using regular expression to catch SKCQ1_1 through SCKQ4_14
  unite(one, contains("1")) %>%  # Unite all SKCQ columns that contain '1' with default sep = "_" into single new column named "one"
  unite(two, contains("2")) %>%
  unite(three, contains("3")) %>%
  unite(four, contains("4")) %>%
  gather(timepoint, values, one:four) %>% # Gather all columns between that named "one" and that named "four" (inclusive) into two new columns: a key column (named "timepoint") and a value column (named "values")
  separate(values, c("solihullConf", "solihullUnd", "solihullUse"), sep = "_") -> datLongSKCQ # separate the column named "values" into two three columns named "solihullConf", "solihullUnd and "solihullUse" according to the separator "_".
datLong <- cbind(datLong, datLongSKCQ)
# Make timepoint variables easier to follow
datLong$timepoint <-
  recode_factor(
    datLong$timepoint,
    one = "1",
    two = "2",
    three = "3",
    four = "4",
    .missing = "NA",
    .ordered = T
  )
# Make new columns numeric
datLong$solihullConf <- as.numeric(datLong$solihullConf)
datLong$solihullUnd <- as.numeric(datLong$solihullUnd)
datLong$solihullUse <- as.numeric(datLong$solihullUse)

# Produce summary of SKCQ data for later graphing
datLongSumm <- ddply(
  datLong,
  c("timepoint"),
  summarise,
  N = sum(!is.na(solihullConf)),
  conf = mean(solihullConf, na.rm = TRUE),
  und = mean(solihullUnd, na.rm = TRUE),
  use = mean(solihullUse, na.rm = TRUE),
  sd   = sd(solihullConf, na.rm = TRUE),
  se   = sd / sqrt(N)
)

```

```{r EDA, include=FALSE}

library(ggplot2)
library(gplots)
library(car)
library(tidyr)
library(plyr)
library(nlme)
library(multcomp)
library(dplyr)
library(psy)
library(gridExtra)
library(psych)
library(pastecs)
library(Hmisc)

# toDo:
# Check in multivariate EDA whether subgroups anticipated to be controlled are individually normal, 
# i.e. school staff vs others. Also check by timepoint for DVs.
# Calculate Levene's test of homogoneity of variances in multivariate EDA for factors, and graphs (Field 7.9.5)
# for correlational analysis.
# Add in the relevant training feedback variables

#################
## Examine IVs ##
#################

indVars <-
  cbind(dat$age, dat$timeAtSchool, dat$timeInJob, deparse.level = 2)

round(stat.desc(indVars, basic = FALSE, norm = TRUE), digits = 3)
# Indicates significant skewness and kurtosis for all three variables

# Check distribution of age
shapiro.test(dat$age)
ggplot(dat, aes(age)) +
  geom_histogram(aes(y = ..density..), colour = "black", fill = "white") +
  stat_function(
    fun = dnorm,
    args = list(
      mean = mean(dat$age, na.rm = TRUE),
      sd = sd(dat$age, na.rm = TRUE)
    ),
    colour = "black",
    size = 1
  ) # Cannot assume normality

dat$ageSqrt <- 
  sqrt(dat$age) # Try square root transformation

shapiro.test(dat$ageSqrt) # Check again
ggplot(dat, aes(ageSqrt)) +
  geom_histogram(aes(y = ..density..), colour = "black", fill = "white") +
  stat_function(
    fun = dnorm,
    args = list(
      mean = mean(dat$ageSqrt, na.rm = TRUE),
      sd = sd(dat$ageSqrt, na.rm = TRUE)
    ),
    colour = "black",
    size = 1
  ) # Cannot assume normality

dat$ageLog <- 
  log(dat$age) # Try log transformation

shapiro.test(dat$ageLog) # Check again
ggplot(dat, aes(ageLog)) +
  geom_histogram(aes(y = ..density..), colour = "black", fill = "white") +
  stat_function(
    fun = dnorm,
    args = list(
      mean = mean(dat$ageLog, na.rm = TRUE),
      sd = sd(dat$ageLog, na.rm = TRUE)
    ),
    colour = "black",
    size = 1
  ) # Cannot assume normality

dat$ageRecip <- 
  1 / (dat$age) # Try reciprocal transformation

shapiro.test(dat$ageRecip) # Check again
ggplot(dat, aes(ageRecip)) +
  geom_histogram(aes(y = ..density..), colour = "black", fill = "white") +
  stat_function(
    fun = dnorm,
    args = list(
      mean = mean(dat$ageRecip, na.rm = TRUE),
      sd = sd(dat$ageRecip, na.rm = TRUE)
    ),
    colour = "black",
    size = 1
  ) # Cannot assume normality
# Age not normally distributed and can't be usefully transformed

# Check distribution of timeAtSchool
shapiro.test(dat$timeAtSchool)
ggplot(dat, aes(timeAtSchool)) +
  geom_histogram(aes(y = ..density..), colour = "black", fill = "white") +
  stat_function(
    fun = dnorm,
    args = list(
      mean = mean(dat$timeAtSchool, na.rm = TRUE),
      sd = sd(dat$timeAtSchool, na.rm = TRUE)
    ),
    colour = "black",
    size = 1
  ) # Cannot assume normality, strong positive skew

dat$timeAtSchoolSqrt <-
  sqrt(dat$timeAtSchool) # Try square root transformation

shapiro.test(dat$timeAtSchoolSqrt) # Check again
ggplot(dat, aes(timeAtSchoolSqrt)) +
  geom_histogram(aes(y = ..density..), colour = "black", fill = "white") +
  stat_function(
    fun = dnorm,
    args = list(
      mean = mean(dat$timeAtSchoolSqrt, na.rm = TRUE),
      sd = sd(dat$timeAtSchoolSqrt, na.rm = TRUE)
    ),
    colour = "black",
    size = 1
  ) # Cannot assume normality

dat$timeAtSchoolLog <-
  log(dat$timeAtSchool + 1) # Try log transformation

shapiro.test(dat$timeAtSchoolLog) # Check again
ggplot(dat, aes(timeAtSchoolLog)) +
  geom_histogram(aes(y = ..density..), colour = "black", fill = "white") +
  stat_function(
    fun = dnorm,
    args = list(
      mean = mean(dat$timeAtSchoolLog, na.rm = TRUE),
      sd = sd(dat$timeAtSchoolLog, na.rm = TRUE)
    ),
    colour = "black",
    size = 1
  ) # Cannot assume normality

dat$timeAtSchoolRecip <-
  (1 / (dat$timeAtSchool + 1)) # Try reciprocal transformation

shapiro.test(dat$timeAtSchoolRecip) # Check again
ggplot(dat, aes(timeAtSchoolRecip)) +
  geom_histogram(aes(y = ..density..), colour = "black", fill = "white") +
  stat_function(
    fun = dnorm,
    args = list(
      mean = mean(dat$timeAtSchoolRecip, na.rm = TRUE),
      sd = sd(dat$timeAtSchoolRecip, na.rm = TRUE)
    ),
    colour = "black",
    size = 1
  ) # Worse
# timeAtSchool is not nomally distributed and can't be usefully transformed

# Check distribution of timeInJob
shapiro.test(dat$timeInJob)
ggplot(dat, aes(timeInJob)) +
  geom_histogram(aes(y = ..density..), colour = "black", fill = "white") +
  stat_function(
    fun = dnorm,
    args = list(
      mean = mean(dat$timeInJob, na.rm = TRUE),
      sd = sd(dat$timeInJob, na.rm = TRUE)
    ),
    colour = "black",
    size = 1
  ) # Cannot assume normality, strong positive skew

dat$timeInJobSqrt <-
  sqrt(dat$timeInJob) # Try square root transformation

shapiro.test(dat$timeInJobSqrt) # Check again
ggplot(dat, aes(timeInJobSqrt)) +
  geom_histogram(aes(y = ..density..), colour = "black", fill = "white") +
  stat_function(
    fun = dnorm,
    args = list(
      mean = mean(dat$timeInJobSqrt, na.rm = TRUE),
      sd = sd(dat$timeInJobSqrt, na.rm = TRUE)
    ),
    colour = "black",
    size = 1
  ) # Cannot assume normality

dat$timeInJobLog <- log(dat$timeInJob + 1) # Try log transformation
shapiro.test(dat$timeInJobLog)

ggplot(dat, aes(timeInJobLog)) +
  geom_histogram(aes(y = ..density..), colour = "black", fill = "white") +
  stat_function(
    fun = dnorm,
    args = list(
      mean = mean(dat$timeInJobLog, na.rm = TRUE),
      sd = sd(dat$timeInJobLog, na.rm = TRUE)
    ),
    colour = "black",
    size = 1
  ) # Better but still negative skew

dat$timeInJobRecip <-
  (1 / (dat$timeInJob + 1)) # Try reciprocal transformation

shapiro.test(dat$timeInJobRecip) # Check again
ggplot(dat, aes(timeInJobRecip)) +
  geom_histogram(aes(y = ..density..), colour = "black", fill = "white") +
  stat_function(
    fun = dnorm,
    args = list(
      mean = mean(dat$timeInJobRecip, na.rm = TRUE),
      sd = sd(dat$timeInJobRecip, na.rm = TRUE)
    ),
    colour = "black",
    size = 1
  ) # Much worse
# timeInJob is not nomally distributed but improved by log transformation

# IV Conclusions
# Univariate normality cannot be assumed for age, timeInSchool and timeInJob
# Transformations don't help very much
# Need to use non-parametric or robust tests when including these control variables in analysis

#################
## Examine DVs ##
#################

# NB can we collapse across timepoints to check normality from 'long format' dataset
# instead of appraoch below?

# Check normality for Professional Quality of Life Scale variables
round(stat.desc(proQolAll, basic = FALSE, norm = TRUE), 3)
# Some skewness for proQolPreSTS and proQolPostSTS, check histograms

proQolBW <- 1 # Set binwidth

# Make histograms
proQolPlot1 <-
  ggplot(dat, aes(proQolPreCS)) +
  geom_histogram(
    aes(y = ..density..),
    colour = "black",
    fill = "white",
    binwidth = proQolBW
  ) +
  stat_function(
    fun = dnorm,
    args = list(
      mean = mean(proQolPreCS, na.rm = TRUE),
      sd = sd(proQolPreCS, na.rm = TRUE)
    ),
    colour = "black",
    size = 1
  )
proQolPlot2 <-
  ggplot(dat, aes(proQolPreBO)) +
  geom_histogram(
    aes(y = ..density..),
    colour = "black",
    fill = "white",
    binwidth = proQolBW
  ) +
  stat_function(
    fun = dnorm,
    args = list(
      mean = mean(proQolPreBO, na.rm = TRUE),
      sd = sd(proQolPreBO, na.rm = TRUE)
    ),
    colour = "black",
    size = 1
  )
proQolPlot3 <-
  ggplot(dat, aes(proQolPreSTS)) +
  geom_histogram(
    aes(y = ..density..),
    colour = "black",
    fill = "white",
    binwidth = proQolBW
  ) +
  stat_function(
    fun = dnorm,
    args = list(
      mean = mean(proQolPreSTS, na.rm = TRUE),
      sd = sd(proQolPreSTS, na.rm = TRUE)
    ),
    colour = "black",
    size = 1
  )
proQolPlot4 <-
  ggplot(dat, aes(proQolPostCS)) +
  geom_histogram(
    aes(y = ..density..),
    colour = "black",
    fill = "white",
    binwidth = proQolBW
  ) +
  stat_function(
    fun = dnorm,
    args = list(
      mean = mean(proQolPostCS, na.rm = TRUE),
      sd = sd(proQolPostCS, na.rm = TRUE)
    ),
    colour = "black",
    size = 1
  )
proQolPlot5 <-
  ggplot(dat, aes(proQolPostBO)) +
  geom_histogram(
    aes(y = ..density..),
    colour = "black",
    fill = "white",
    binwidth = proQolBW
  ) +
  stat_function(
    fun = dnorm,
    args = list(
      mean = mean(proQolPostBO, na.rm = TRUE),
      sd = sd(proQolPostBO, na.rm = TRUE)
    ),
    colour = "black",
    size = 1
  )
proQolPlot6 <-
  ggplot(dat, aes(proQolPostSTS)) +
  geom_histogram(
    aes(y = ..density..),
    colour = "black",
    fill = "white",
    binwidth = proQolBW
  ) +
  stat_function(
    fun = dnorm,
    args = list(
      mean = mean(proQolPostSTS, na.rm = TRUE),
      sd = sd(proQolPostSTS, na.rm = TRUE)
    ),
    colour = "black",
    size = 1
  )
grid.arrange(proQolPlot1,
             proQolPlot2,
             proQolPlot3,
             proQolPlot4,
             proQolPlot5,
             proQolPlot6,
             ncol = 3) # Display histograms

# Accept normal distributions for PROQOL variables for now, need
# to check when combining across pre and post timepoints


# Check normality for SKCQ variables
round(stat.desc(
  cbind(
    datLong$solihullConf,
    datLong$solihullUnd,
    datLong$solihullUse,
    deparse.level = 2
  ),
  basic = FALSE,
  norm = TRUE
), 3)
# Significant negative skewness for all three varialbes
# Check histograms
SKCQLongBW <- 0.3 # Set the bin width
SKCQLongPlot1 <-
  ggplot(datLong, aes(datLong$solihullConf)) +
  geom_histogram(
    aes(y = ..density..),
    colour = "black",
    fill = "white",
    binwidth = SKCQLongBW
  ) +
  stat_function(
    fun = dnorm,
    args = list(
      mean = mean(datLong$solihullConf, na.rm = TRUE),
      sd = sd(datLong$solihullConf, na.rm = TRUE)
    ),
    colour = "black",
    size = 1
  )
SKCQLongPlot2 <-
  ggplot(datLong, aes(datLong$solihullUnd)) +
  geom_histogram(
    aes(y = ..density..),
    colour = "black",
    fill = "white",
    binwidth = SKCQLongBW
  ) +
  stat_function(
    fun = dnorm,
    args = list(
      mean = mean(datLong$solihullUnd, na.rm = TRUE),
      sd = sd(datLong$solihullUnd, na.rm = TRUE)
    ),
    colour = "black",
    size = 1
  )
SKCQLongPlot3 <-
  ggplot(datLong, aes(datLong$solihullUse)) +
  geom_histogram(
    aes(y = ..density..),
    colour = "black",
    fill = "white",
    binwidth = SKCQLongBW
  ) +
  stat_function(
    fun = dnorm,
    args = list(
      mean = mean(datLong$solihullUse, na.rm = TRUE),
      sd = sd(datLong$solihullUse, na.rm = TRUE)
    ),
    colour = "black",
    size = 1
  )
grid.arrange(SKCQLongPlot1,
             SKCQLongPlot2,
             SKCQLongPlot3,
             ncol = 3)
# Cannot assume normality for SKCQ variables when collapsed across timepoints

# Check whether SKCQ variables are normally distributed at each timepoint
round(stat.desc(SKCQAll, basic = FALSE, norm = TRUE), 3)
# Cannot assume normality for most variables, check histograms
# Histograms for SKCQ variables
SKCQBW <- 0.2
SKCQPlot1 <-
  ggplot(dat, aes(SKCQ1Conf)) + geom_histogram(binwidth = SKCQBW) # Assume normal distribution
SKCQPlot2 <-
  ggplot(dat, aes(SKCQ2Conf)) + geom_histogram(binwidth = SKCQBW) # Assume normal distribution
SKCQPlot3 <-
  ggplot(dat, aes(SKCQ3Conf)) + geom_histogram(binwidth = SKCQBW) # Assume normal distribution
SKCQPlot4 <-
  ggplot(dat, aes(SKCQ4Conf)) + geom_histogram(binwidth = SKCQBW) # Cannot assume normality
SKCQPlot5 <-
  ggplot(dat, aes(SKCQ1Und)) + geom_histogram(binwidth = SKCQBW) # Assume normal distribution
SKCQPlot6 <-
  ggplot(dat, aes(SKCQ2Und)) + geom_histogram(binwidth = SKCQBW) # Assume normal distribution
SKCQPlot7 <-
  ggplot(dat, aes(SKCQ3Und)) + geom_histogram(binwidth = SKCQBW) # Assume normal distribution
SKCQPlot8 <-
  ggplot(dat, aes(SKCQ4Und)) + geom_histogram(binwidth = SKCQBW) # Assume normal distribution
SKCQPlot9 <-
  ggplot(dat, aes(SKCQ1Use)) + geom_histogram(binwidth = SKCQBW) # Cannot assume normality
SKCQPlot10 <-
  ggplot(dat, aes(SKCQ2Use)) + geom_histogram(binwidth = SKCQBW) # Assume normal distribution
SKCQPlot11 <-
  ggplot(dat, aes(SKCQ3Use)) + geom_histogram(binwidth = SKCQBW) # Assume normal distribution
SKCQPlot12 <-
  ggplot(dat, aes(SKCQ4Use)) + geom_histogram(binwidth = SKCQBW) # Assume normal distribution
grid.arrange(
  SKCQPlot1,
  SKCQPlot2,
  SKCQPlot3,
  SKCQPlot4,
  SKCQPlot5,
  SKCQPlot6,
  SKCQPlot7,
  SKCQPlot8,
  SKCQPlot9,
  SKCQPlot10,
  SKCQPlot11,
  SKCQPlot12,
  ncol = 4
)
# Statistical check for SKCQ variables appearing to deviate from normality
shapiro.test(dat$SKCQ4Conf)
# Not normally distributed, try transformations

# Tranformations for SKCQ variables violating assumption of normality
SKCQ4ConfSqrt <-
  sqrt(SKCQ4Conf) # Try square root transformation for SKCQ4Conf
shapiro.test(SKCQ4ConfSqrt)
ggplot(dat, aes(SKCQ4ConfSqrt)) + geom_histogram() # Cannot assume normality
SKCQ4ConfLog <-
  log(SKCQ4Conf) # Try log transformation for SKCQ4Conf
shapiro.test(SKCQ4ConfLog)
ggplot(dat, aes(SKCQ4ConfLog)) + geom_histogram() # Cannot assume normality
SKCQ4ConfRecip <-
  1 / (SKCQ4Conf) # Try reciprocal transformation for SKCQ4Conf
shapiro.test(SKCQ4ConfRecip)
ggplot(dat, aes(SKCQ4ConfRecip)) + geom_histogram() # Cannot assume normality
# SKCQ4Conf not normally distributed and can't be usefully transformed
SKCQ1UseSqrt <-
  sqrt(SKCQ1Use) # Try square root transformation for SKCQ1Use
shapiro.test(SKCQ1UseSqrt)
ggplot(dat, aes(SKCQ1UseSqrt)) + geom_histogram() # Cannot assume normality
SKCQ1UseLog <- log(SKCQ1Use) # Try log transformation for SKCQ1Use
shapiro.test(SKCQ1UseLog)
ggplot(dat, aes(SKCQ1UseLog)) + geom_histogram() # Cannot assume normality
SKCQ1UseRecip <-
  1 / (SKCQ1Use) # Try reciprocal transformation for SKCQ1Use
shapiro.test(SKCQ1UseRecip)
ggplot(dat, aes(SKCQ1UseRecip)) + geom_histogram() # Cannot assume normality
# SKCQ1Use not normally distributed and can't be usefully transformed
# Use non-parametric methods with SKCQ data

# Check internal consistency for Solihull Knowlege and Confidence Questionnaire
SKCQ1ConfCols <-
  cbind(dat$SKCQ1_1, dat$SKCQ1_2, dat$SKCQ1_3, dat$SKCQ1_4) # Combine columns for each Confidence subscale administration
SKCQ2ConfCols <-
  cbind(dat$SKCQ2_1, dat$SKCQ2_2, dat$SKCQ2_3, dat$SKCQ2_4)
SKCQ3ConfCols <-
  cbind(dat$SKCQ2_1, dat$SKCQ3_2, dat$SKCQ3_3, dat$SKCQ3_4)
SKCQ4ConfCols <-
  cbind(dat$SKCQ4_1, dat$SKCQ4_2, dat$SKCQ4_3, dat$SKCQ4_4)
SKCQ1UndCols <-
  cbind(dat$SKCQ1_5,
        dat$SKCQ1_6,
        dat$SKCQ1_7,
        dat$SKCQ1_8,
        dat$SKCQ1_9) # Combine columns for each Undersatnding subscale administration
SKCQ2UndCols <-
  cbind(dat$SKCQ2_5,
        dat$SKCQ2_6,
        dat$SKCQ2_7,
        dat$SKCQ2_8,
        dat$SKCQ2_9)
SKCQ3UndCols <-
  cbind(dat$SKCQ3_5,
        dat$SKCQ3_6,
        dat$SKCQ3_7,
        dat$SKCQ3_8,
        dat$SKCQ3_9)
SKCQ4UndCols <-
  cbind(dat$SKCQ4_5,
        dat$SKCQ4_6,
        dat$SKCQ4_7,
        dat$SKCQ4_8,
        dat$SKCQ4_9)
SKCQ1UseCols <-
  cbind(dat$SKCQ1_10,
        dat$SKCQ1_11,
        dat$SKCQ1_12,
        dat$SKCQ1_13,
        dat$SKCQ1_14) # Combine columns for each Understanding subscale administration
SKCQ2UseCols <-
  cbind(dat$SKCQ2_10,
        dat$SKCQ2_11,
        dat$SKCQ2_12,
        dat$SKCQ2_13,
        dat$SKCQ2_14)
SKCQ3UseCols <-
  cbind(dat$SKCQ3_10,
        dat$SKCQ3_11,
        dat$SKCQ3_12,
        dat$SKCQ3_13,
        dat$SKCQ3_14)
SKCQ4UseCols <-
  cbind(dat$SKCQ4_10,
        dat$SKCQ4_11,
        dat$SKCQ4_12,
        dat$SKCQ4_13,
        dat$SKCQ4_14)
cronbach(SKCQ1ConfCols) # Calculate Cronbach's alpha for SKCQ Confidence items
cronbach(SKCQ2ConfCols)
cronbach(SKCQ3ConfCols)
cronbach(SKCQ4ConfCols)
cronbach(SKCQ1UndCols) # Calculate Cronbach's alpha for SKCQ Understanding items
cronbach(SKCQ2UndCols)
cronbach(SKCQ3UndCols)
cronbach(SKCQ4UndCols)
cronbach(SKCQ1UseCols) # Calculate Cronbach's alpha for SKCQ Usage items
cronbach(SKCQ2UseCols)
cronbach(SKCQ3UseCols)
cronbach(SKCQ4UseCols)
# Acceptable internal consistency for SKCQ variables EXCEPT SKCQ3Conf. Fewest participants completed this one.

# Produce summary of SKCQ data for later graphing
datLongSumm <- ddply(
  datLong,
  c("timepoint"),
  summarise,
  N = sum(!is.na(solihullConf)),
  conf = mean(solihullConf, na.rm = TRUE),
  und = mean(solihullUnd, na.rm = TRUE),
  use = mean(solihullUse, na.rm = TRUE),
  sd   = sd(solihullConf, na.rm = TRUE),
  se   = sd / sqrt(N)
)

######################
## Multivariate EDA ##
######################

# Correlation matrices for Solihull Knowledge and Confidence Questionnaire
rcorr(as.matrix(SKCQAll), type = "pearson")
# All SKCQ subscales significantly intercorrelated at baseline and follow-up

# Check for effects of school on baseline DVs

# Check whether SKCQ scales vary by school
ggplot(dat, aes(factor(school), SKCQ1Conf)) +
  geom_boxplot()
ggplot(dat, aes(factor(school), SKCQ1Und)) +
  geom_boxplot()
ggplot(dat, aes(factor(school), SKCQ1Use)) +
  geom_boxplot()
fit <-
  aov(SKCQ1Conf ~ school, data = dat) # One Way Anova (Completely Randomized Design)
layout(matrix(c(1, 2, 3, 4), 2, 2)) # optional layout
plot(fit) # diagnostic plots
summary(fit) # display Type I ANOVA table
with(dat,
     pairwise.t.test(
       SKCQ1Conf,
       school,
       p.adj = "bonferroni",
       paired = F,
       na.rm = T
     ))
# School 1 significantly lower pre-training SKCQ Confidence score than school 2.
fit <-
  aov(SKCQ1Und ~ school, data = dat) # One Way Anova (Completely Randomized Design)
layout(matrix(c(1, 2, 3, 4), 2, 2)) # optional layout
plot(fit) # diagnostic plots
summary(fit) # display Type I ANOVA table
with(dat,
     pairwise.t.test(
       SKCQ1Und,
       school,
       p.adj = "bonferroni",
       paired = F,
       na.rm = T
     ))
# No significant effect of school on baseline SCKQUnd
fit <-
  aov(SKCQ1Use ~ school, data = dat) # One Way Anova (Completely Randomized Design)
layout(matrix(c(1, 2, 3, 4), 2, 2)) # optional layout
plot(fit) # diagnostic plots
summary(fit) # display Type I ANOVA table
with(dat,
     pairwise.t.test(
       SKCQ1Use,
       school,
       p.adj = "bonferroni",
       paired = F,
       na.rm = T
     ))
# No significant effect of school on baseline SCKQUse

# Check visually whether PROQOL subscales vary by school
ggplot(dat, aes(factor(dat$school), proQolPreCST)) +
  geom_boxplot()
ggplot(dat, aes(factor(dat$school), proQolPreBOT)) +
  geom_boxplot()
ggplot(dat, aes(factor(dat$school), proQolPreSTST)) +
  geom_boxplot()
# Check statistically whether PROQOL subscales vary by school
fit <-
  aov(proQolPreCS ~ school, data = dat) # One Way Anova (Completely Randomized Design)
layout(matrix(c(1, 2, 3, 4), 2, 2)) # optional layout
plot(fit) # diagnostic plots
summary(fit) # Significant effect of school on baseline PROQOL Compassion scale
with(dat,
     pairwise.t.test(
       proQolPreCS,
       school,
       p.adj = "bonferroni",
       paired = F,
       na.rm = T
     ))
fit <-
  aov(proQolPreBO ~ school, data = dat) # One Way Anova (Completely Randomized Design)
layout(matrix(c(1, 2, 3, 4), 2, 2)) # optional layout
plot(fit) # diagnostic plots
summary(fit) # Significant effect of school on baseline PROQOL Burnout scale
with(dat,
     pairwise.t.test(
       proQolPreBO,
       school,
       p.adj = "bonferroni",
       paired = F,
       na.rm = T
     ))
fit <-
  aov(proQolPreSTS ~ school, data = dat) # One Way Anova (Completely Randomized Design)
layout(matrix(c(1, 2, 3, 4), 2, 2)) # optional layout
plot(fit) # diagnostic plots
summary(fit) # No effect of school on baseline PROQOL Secondary Traumatic Stress scale
with(dat,
     pairwise.t.test(
       proQolPreSTS,
       school,
       p.adj = "bonferroni",
       paired = F,
       na.rm = T
     ))
# Significant effects of school on baseline DVs
# Include school as control variable in later analysis

datSchOnly <- dat[dat$school %in% c(2:5), ] # Works to select only school-based training sites

# Check for effects of job on baseline DVs

# Check visually whether SKCQ scales vary by job
ggplot(dat, aes(factor(dat$jobCat), SKCQ1Conf)) +
  geom_boxplot()
ggplot(dat, aes(factor(dat$jobCat), SKCQ1Und)) +
  geom_boxplot()
ggplot(dat, aes(factor(dat$jobCat), SKCQ1Use)) +
  geom_boxplot()
# Examine whether SKCQ scales vary by job
# Check Confidence using ANOVA
fitSKCQConf1Job <-
  aov(SKCQ1Conf ~ dat$jobCat, data = dat) # One Way Anova (Completely Randomized Design)
layout(matrix(c(1, 2, 3, 4), 2, 2)) # optional layout
plot(fitSKCQConf1Job) # diagnostic plots
summary(fitSKCQConf1Job) # display Type I ANOVA table
with(dat,
     pairwise.t.test(
       SKCQ1Conf,
       dat$jobCat,
       p.adj = "bonferroni",
       paired = F,
       na.rm = T
     ))
# SLT's baseline SKCQ Confidence scores were significantly lower than teachers, TAs and 'others' but not FSWs
# Check Understanding using ANOVA
fitSKCQUnd1Job <-
  aov(SKCQ1Und ~ dat$jobCat, data = dat) # One Way Anova (Completely Randomized Design)
layout(matrix(c(1, 2, 3, 4), 2, 2)) # optional layout
plot(fitSKCQUnd1Job) # diagnostic plots
summary(fitSKCQUnd1Job) # display Type I ANOVA table
# No main effects of job on baseline SKCQ Understanding scores
# Check Usage using ANOVA
fitSKCQUse1Job <-
  aov(SKCQ1Use ~ dat$jobCat, data = dat) # One Way Anova (Completely Randomized Design)
layout(matrix(c(1, 2, 3, 4), 2, 2)) # optional layout
plot(fitSKCQUse1Job) # diagnostic plots
summary(fitSKCQUse1Job) # display Type I ANOVA table
# No main effects of job on baseline SKCQ Usage scores

# Check visually whether PROQOL scales vary by job
# NB 'school 1' did not complete PROQOL
ggplot(dat, aes(factor(dat$jobCat), proQolPreCS)) +
  geom_boxplot()
ggplot(dat, aes(factor(dat$jobCat), proQolPreBO)) +
  geom_boxplot()
ggplot(dat, aes(factor(dat$jobCat), proQolPreSTS)) +
  geom_boxplot()
# Check proQolPreCS using ANOVA
fitProQOLPreCSJob <-
  aov(proQolPreCS ~ dat$jobCat, data = dat) # One Way Anova (Completely Randomized Design)
layout(matrix(c(1, 2, 3, 4), 2, 2)) # optional layout
plot(fitProQOLPreCSJob) # diagnostic plots
summary(fitProQOLPreCSJob) # display Type I ANOVA table
# No main effects of job on baseline PROQOL CS scale
# Check proQolPreBO using ANOVA
fitProQOLPreBOJob <-
  aov(proQolPreBO ~ dat$jobCat, data = dat) # One Way Anova (Completely Randomized Design)
layout(matrix(c(1, 2, 3, 4), 2, 2)) # optional layout
plot(fitProQOLPreBOJob) # diagnostic plots
summary(fitProQOLPreBOJob) # display Type I ANOVA table
# No main effects of job on baseline PROQOL BO scale
# Check proQolPreSTS using ANOVA
fitProQOLPreSTSJob <-
  aov(proQolPreSTS ~ dat$jobCat, data = dat) # One Way Anova (Completely Randomized Design)
layout(matrix(c(1, 2, 3, 4), 2, 2)) # optional layout
plot(fitProQOLPreSTSJob) # diagnostic plots
summary(fitProQOLPreSTSJob) # display Type I ANOVA table
# No main effects of job on baseline PROQOL BO scale

# Need to control for job during analysis of training effects.
```

# Data Analysis

## Hypothesis 1
### Confidence with emotional/behavioural difficulties, understanding of Solihull concepts and intent to use Solihull concepts all increase following Solihull training.

Compare confidence in working with emotional/behavioural difficulties at T1 and T4:
```{r include=TRUE}

wilcox.test(
  SKCQ1Conf,
  SKCQ4Conf,
  alternative = "two.sided",
  paired = TRUE,
  exact = FALSE,
  conf.int = TRUE
)
```

Compare understanding of Solihull concepts at T1 and T4:

```{r include = TRUE}
wilcox.test(
  SKCQ1Und,
  SKCQ4Und,
  alternative = "two.sided",
  paired = TRUE,
  exact = FALSE,
  conf.int = TRUE
)
```

Compare intent to use Solihull concepts at T1 and T4:

```{r include = TRUE}
wilcox.test(
  SKCQ1Use,
  SKCQ4Use,
  alternative = "two.sided",
  paired = TRUE,
  exact = FALSE,
  conf.int = TRUE
)
```

All differences are significant according to non-parametric tests.

#### Hypothesis 1 accepted.

## Hypothesis 2
### Professional quality of life increases following Solihull training.

```{r include = TRUE}
wilcox.test(
  proQolPreCS,
  proQolPostCS,
  alternative = "two.sided",
  paired = TRUE,
  exact = FALSE,
  conf.int = TRUE
)
```
Difference is significant according to non-parametric test.

#### Hypothesis 2 accepted.

## Hypothesis 3
### Solihull training effects are associated with increased professional quality of life

Test for linear relationships between change in SKCQ and change in PROQOL first by examining all correlations (not shown), then follow up significant ones.

```{r include=FALSE}
rcorr(as.matrix(data.frame(SKCQAll, proQolAll), type = "spearman"))
```

Significant correlation between T2 intent to use Solihull concepts and post-training PROQOL compassion satisfaction:

```{r include = TRUE}
cor.test(SKCQ2Use, proQolPostCS, method = "spearman")
```

Significant correlation between T2 intent to use Solihull concepts and post-training PROQOL burnout:

```{r include = TRUE}
cor.test(SKCQ2Use, proQolPostBO, method = "spearman")
```

What about controlling for baseline PROQOL?
Correlate change scores for SKCQ and PROQOL variables:

```{r, include = TRUE}
ChgScores <-
  data.frame(dat$SKCQConfChg,
             dat$SKCQUndChg,
             dat$SKCQUseChg,
             dat$proQolCSChg,
             dat$proQolBOChg,
             dat$proQolSTSChg)
rcorr(as.matrix(ChgScores), type = "spearman")
```
No significant correlations of change scores

#### Hypothesis 3 not accepted.

# Plots

```{r plot, include = TRUE}

# toDo:
# pre/post graph for PROQOL variables once sorted out 'long' format data in preparation.R

# Plot change in SCKQ variables
pd <- position_dodge(width = 0.5)
ggplot(datLongSumm, aes(x = timepoint, group = 1)) +
  geom_line(aes(y = conf, colour = "conf", group = 1), position = pd) +
  geom_line(aes(y = und, colour = "und", group = 1), position = pd) +
  geom_line(aes(y = use, colour = "use", group = 1), position = pd) +
  geom_point(aes(y = conf, colour = "conf", group = 1),
             position = pd,
             size = 2) +
  geom_point(aes(y = und, colour = "und", group = 1),
             position = pd,
             size = 2) +
  geom_point(aes(y = use, colour = "use", group = 1),
             position = pd,
             size = 2) +
  geom_errorbar(
    aes(
      ymin = conf - se,
      ymax = conf + se,
      colour = "conf",
      group = 1
    ),
    width = .1,
    position = pd
  ) +
  geom_errorbar(
    aes(
      ymin = und - se,
      ymax = und + se,
      colour = "und",
      group = 1
    ),
    width = .1,
    position = pd
  ) +
  geom_errorbar(
    aes(
      ymin = use - se,
      ymax = use + se,
      colour = "use",
      group = 1
    ),
    width = .1,
    position = pd
  ) +
  ylab("Rating") +
  xlab("Time") +
  scale_x_discrete(
    breaks = c("1", "2", "3", "4"),
    labels = c(
      "Before",
      "After\nwhole day",
      "After\ntwilight",
      "After\n6 months"
    )
  ) +
  ggtitle("Solihull Training Effects") +
  scale_colour_discrete(name  = "Subscale",
                        labels = c("Confidence", "Understanding", "Usage")) +
  theme(legend.justification = "right",
        legend.position = c(1, 0.17))
```
